{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 3: Plausible Utopia \n",
    "\n",
    "### Classification Modeling on Subreddits to Classify Futurists vs. Scientists\n",
    "\n",
    "### Notebook 2 of 3: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "     \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "##this will hide deprecation/future warnings\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "pd.set_option('display.max_row', 200) # Set ipython's max row display\n",
    "pd.set_option('display.max_columns', 85) # Set iPython's max column count\n",
    "pd.set_option('display.max_colwidth', 1_000) # Set iPython's max column width\n",
    "\n",
    "# pseudo-markdown in code cells\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "# ref: https://discuss.analyticsvidhya.com/t/how-to-make-a-text-bold-within-print-statement-in-ipython-notebook/14552/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>futurology</td>\n",
       "      <td>what would the point be to do anyth if ai could just do it all for us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>futurology</td>\n",
       "      <td>elcc explain the critic renew energi concept you’v never heard of now we live in a day and age where some part of the countri gener more than 30 of their electr with variabl renew and that number will fast approach 100 in the decad to come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>futurology</td>\n",
       "      <td>there a 5050 chanc were live in a simul and here how to find out the probabl will increas as we develop technolog enabl the creation of a simul contain consciou be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>futurology</td>\n",
       "      <td>eight nation sign nasa artemi accord pledg peac on the moon it a reaffirm of the 1967 outer space treati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>futurology</td>\n",
       "      <td>merced benz avtr  in action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit  \\\n",
       "0  futurology   \n",
       "1  futurology   \n",
       "2  futurology   \n",
       "3  futurology   \n",
       "4  futurology   \n",
       "\n",
       "                                                                                                                                                                                                                                              title  \n",
       "0                                                                                                                                                                            what would the point be to do anyth if ai could just do it all for us   \n",
       "1  elcc explain the critic renew energi concept you’v never heard of now we live in a day and age where some part of the countri gener more than 30 of their electr with variabl renew and that number will fast approach 100 in the decad to come   \n",
       "2                                                                              there a 5050 chanc were live in a simul and here how to find out the probabl will increas as we develop technolog enabl the creation of a simul contain consciou be   \n",
       "3                                                                                                                                         eight nation sign nasa artemi accord pledg peac on the moon it a reaffirm of the 1967 outer space treati   \n",
       "4                                                                                                                                                                                                                      merced benz avtr  in action   "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data\n",
    "subred = pd.read_csv('../data/cleaned_data/cleaned_df.csv')\n",
    "\n",
    "# Check its shape\n",
    "display(subred.shape)\n",
    "\n",
    "# Preview\n",
    "subred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set feature matrix and target variable\n",
    "\n",
    "X = subred['title']\n",
    "y = subred['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read the following from ISLR:\n",
    "\n",
    "5.2 - The Bootstrap. (You'll want to know the bootstrap well as it will be embedded in methods we'll discuss next week.)\n",
    "Chapter 8 - Tree Based Methods. (We'll learn about classification and regression trees, random forests, bagging and boosting next week.)\n",
    "9.1-9.5 (Support Vector Machines and related methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify an optimal model with a pipeline and GridSearch\n",
    "\n",
    "### Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Ref: 5.05 lesson**\n",
    " \n",
    " Features can be completely different in train/test\n",
    " \n",
    "**IMPORTANT** Instead of splitting we can use entire dataset combined with cross-validation\n",
    " \n",
    " Depending on how big dataset is, significant words in train set will probably appear in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1206,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1206,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(594,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(594,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.33,\n",
    "                                                    stratify = y,\n",
    "                                                    random_state = 42)\n",
    "\n",
    "display(X_train.shape)\n",
    "display(y_train.shape)\n",
    "\n",
    "display(X_test.shape)\n",
    "display(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline accuracy\n",
    "\n",
    "**51.7% is the baseline accuracy percentage we compare with the model's accuracy.**\n",
    "\n",
    "**If the model does better than the baseline, then it is better than null model (predicting the majority / plurality class).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "science       0.517222\n",
       "futurology    0.482778\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "science       0.517413\n",
       "futurology    0.482587\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "science       0.516835\n",
       "futurology    0.483165\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(y.value_counts(normalize=True))\n",
    "\n",
    "display(y_train.value_counts(normalize=True))\n",
    "\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutoff point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 1\n",
    "\n",
    "**CountVectorizer (transformer)**:\n",
    "\n",
    "\n",
    "\n",
    "**Multinomial Naive Bayes (estimator)**:\n",
    "\n",
    "* Fast modeling algorithm\n",
    "* Variables are positive integers\n",
    "* Assumes all variables are independent (which is not realistic in NLP, but this assumption is accepted anyway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. CountVectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "\n",
    "pipe_cvec_nb = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())    # Naive Bayes\n",
    "])\n",
    "\n",
    "\n",
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit\n",
    "# Minimum number of documents needed to include token\n",
    "# Maximum number of documents needed to include token\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "pipe_cvec_nb_params = {\n",
    "    'cvec__max_features': [1_000, 2_000, 3_000, 4_000, 5_000],\n",
    "    'cvec__min_df': [1, 2],\n",
    "    'cvec__max_df': [.98, .99],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "\n",
    "gs_cvec_nb = GridSearchCV(pipe_cvec_nb, # what object are we optimizing?\n",
    "                  param_grid = pipe_cvec_nb_params, # what parameters values are we searching?\n",
    "                  cv=5)      # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             param_grid={'cvec__max_df': [0.98, 0.99],\n",
       "                         'cvec__max_features': [1000, 2000, 3000, 4000, 5000],\n",
       "                         'cvec__min_df': [1, 2],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to data.\n",
    "gs_cvec_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Grid searched:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_features': [1000, 2000, 3000, 4000, 5000],\n",
       " 'cvec__min_df': [1, 2],\n",
       " 'cvec__max_df': [0.98, 0.99],\n",
       " 'cvec__ngram_range': [(1, 1), (1, 2)]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Best accuracy score**: 0.8449"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Best estimator / parameters:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.98, max_features=4000,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('nb', MultinomialNB())])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.98,\n",
       " 'cvec__max_features': 4000,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_cvec__max_df</th>\n",
       "      <th>param_cvec__max_features</th>\n",
       "      <th>param_cvec__min_df</th>\n",
       "      <th>param_cvec__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.059890</td>\n",
       "      <td>0.004141</td>\n",
       "      <td>0.011228</td>\n",
       "      <td>0.004603</td>\n",
       "      <td>0.99</td>\n",
       "      <td>4000</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'cvec__max_df': 0.99, 'cvec__max_features': 4000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 2)}</td>\n",
       "      <td>0.859504</td>\n",
       "      <td>0.838174</td>\n",
       "      <td>0.875519</td>\n",
       "      <td>0.804979</td>\n",
       "      <td>0.846473</td>\n",
       "      <td>0.844930</td>\n",
       "      <td>0.023631</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.127961</td>\n",
       "      <td>0.008736</td>\n",
       "      <td>0.018089</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>0.98</td>\n",
       "      <td>4000</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'cvec__max_df': 0.98, 'cvec__max_features': 4000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 2)}</td>\n",
       "      <td>0.859504</td>\n",
       "      <td>0.838174</td>\n",
       "      <td>0.875519</td>\n",
       "      <td>0.804979</td>\n",
       "      <td>0.846473</td>\n",
       "      <td>0.844930</td>\n",
       "      <td>0.023631</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.024478</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>0.99</td>\n",
       "      <td>3000</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'cvec__max_df': 0.99, 'cvec__max_features': 3000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 1)}</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>0.846473</td>\n",
       "      <td>0.875519</td>\n",
       "      <td>0.788382</td>\n",
       "      <td>0.846473</td>\n",
       "      <td>0.839964</td>\n",
       "      <td>0.028350</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.049951</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.010269</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3000</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'cvec__max_df': 0.98, 'cvec__max_features': 3000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 1)}</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>0.846473</td>\n",
       "      <td>0.875519</td>\n",
       "      <td>0.788382</td>\n",
       "      <td>0.846473</td>\n",
       "      <td>0.839964</td>\n",
       "      <td>0.028350</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.021115</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>4000</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'cvec__max_df': 0.99, 'cvec__max_features': 4000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 1)}</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>0.854772</td>\n",
       "      <td>0.867220</td>\n",
       "      <td>0.788382</td>\n",
       "      <td>0.842324</td>\n",
       "      <td>0.839134</td>\n",
       "      <td>0.026960</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "33       0.059890      0.004141         0.011228        0.004603   \n",
       "13       0.127961      0.008736         0.018089        0.003119   \n",
       "28       0.024478      0.003177         0.003167        0.003207   \n",
       "8        0.049951      0.000637         0.010269        0.000127   \n",
       "32       0.021115      0.002558         0.005106        0.003001   \n",
       "\n",
       "   param_cvec__max_df param_cvec__max_features param_cvec__min_df  \\\n",
       "33               0.99                     4000                  1   \n",
       "13               0.98                     4000                  1   \n",
       "28               0.99                     3000                  1   \n",
       "8                0.98                     3000                  1   \n",
       "32               0.99                     4000                  1   \n",
       "\n",
       "   param_cvec__ngram_range  \\\n",
       "33                  (1, 2)   \n",
       "13                  (1, 2)   \n",
       "28                  (1, 1)   \n",
       "8                   (1, 1)   \n",
       "32                  (1, 1)   \n",
       "\n",
       "                                                                                                params  \\\n",
       "33  {'cvec__max_df': 0.99, 'cvec__max_features': 4000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 2)}   \n",
       "13  {'cvec__max_df': 0.98, 'cvec__max_features': 4000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 2)}   \n",
       "28  {'cvec__max_df': 0.99, 'cvec__max_features': 3000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 1)}   \n",
       "8   {'cvec__max_df': 0.98, 'cvec__max_features': 3000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 1)}   \n",
       "32  {'cvec__max_df': 0.99, 'cvec__max_features': 4000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 1)}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "33           0.859504           0.838174           0.875519   \n",
       "13           0.859504           0.838174           0.875519   \n",
       "28           0.842975           0.846473           0.875519   \n",
       "8            0.842975           0.846473           0.875519   \n",
       "32           0.842975           0.854772           0.867220   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "33           0.804979           0.846473         0.844930        0.023631   \n",
       "13           0.804979           0.846473         0.844930        0.023631   \n",
       "28           0.788382           0.846473         0.839964        0.028350   \n",
       "8            0.788382           0.846473         0.839964        0.028350   \n",
       "32           0.788382           0.842324         0.839134        0.026960   \n",
       "\n",
       "    rank_test_score  \n",
       "33                1  \n",
       "13                1  \n",
       "28                3  \n",
       "8                 3  \n",
       "32                5  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printmd('**Grid searched:**')\n",
    "display(gs_cvec_nb.param_grid)\n",
    "printmd(f'**Best accuracy score**: {round(gs_cvec_nb.best_score_, 4)}')\n",
    "printmd('**Best estimator / parameters:**')\n",
    "display(gs_cvec_nb.best_estimator_)\n",
    "display(gs_cvec_nb.best_params_)\n",
    "\n",
    "pd.DataFrame(gs_cvec_nb.cv_results_).sort_values('rank_test_score').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "credit: NLP II: `CountVectorizer`, `TfidfVectorizer`, and Modeling lesson. _Authors: Dave Yerrington (SF), Justin Pounders (ATL), Riley Dallas (ATX), Matt Brems (DC), Noelle Brown (DEN)_"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions\n",
    "preds = gs_cvec_nb.predict(X_test)\n",
    "\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "\n",
    "# View confusion matrix\n",
    "\n",
    "plot_confusion_matrix(gs_cvec_nb, X_test, y_test, cmap='cubehelix', values_format='d');\n",
    "plt.title('Confusion matrix')\n",
    "\n",
    "printmd('credit: NLP II: `CountVectorizer`, `TfidfVectorizer`, and Modeling lesson. _Authors: Dave Yerrington (SF), Justin Pounders (ATL), Riley Dallas (ATX), Matt Brems (DC), Noelle Brown (DEN)_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8400673400673401\n",
      "Misclassification: 0.15993265993265993\n",
      "Sensitivity: 0.8534201954397395\n",
      "Precision: 0.8397435897435898\n",
      "Specificity: 0.8257839721254355\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy\n",
    "\n",
    "acc = (tp + tn) / (tp + fp + tn + fn)\n",
    "print('Accuracy:', acc)\n",
    "\n",
    "# Calculate the misclassification rate\n",
    "\n",
    "misclass = (fp + fn) / (tp + fp + tn + fn)\n",
    "print('Misclassification:', misclass)\n",
    "\n",
    "# Calculate the sensitivity\n",
    "\n",
    "sens = tp / (tp + fn)\n",
    "print('Sensitivity:', sens)\n",
    "\n",
    "# Calculate the precision\n",
    "\n",
    "prec = tp / (tp + fp)\n",
    "print('Precision:', prec)\n",
    "\n",
    "# Calculate the specificity\n",
    "\n",
    "spec = tn / (tn + fp)\n",
    "print('Specificity:', spec)\n",
    "\n",
    "def metrics(y_test, y_predict, pred_proba):\n",
    "    print('Accuracy score %s ' % accuracy_score(y_test, y_predict), '\\n')\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, y_predict), \n",
    "                            index=['Not Actual Disaster', 'Actual Disaster'], \n",
    "                            columns=['Predicted Not Actual Disaster', 'Predicted Disaster']), '\\n')\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('ROC_AUC SCORE %s ' %roc_auc_score(y_test, pred_proba))\n",
    "    \n",
    "# credit: Heather Robinson :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the scoreboard or not? These are artificially high scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model # |     Model metrics     |  Score | Before preprocessing shape | train_test_split  | Value |   Pipeline  |        pipe_cvec_nb       | Pipe hyperparameters |                         Value set                         | GridsearchCV parameters | Value | GridsearchCV BEST parameters | Value  |\n",
    "|:-------:|:---------------------:|:------:|:--------------------------:|:-----------------:|:-----:|:-----------:|:-------------------------:|:--------------------:|:---------------------------------------------------------:|:-----------------------:|:-----:|:----------------------------:|--------|\n",
    "|    1    |       Best score      | 0.8389 |          (1800, 7)         |     test_size     |  0.33 | Transformer | 'cvec', CountVectorizer() |     max_features     | 'cvec__max_features': [1_000, 2_000, 3_000, 4_000, 5_000] | k-fold cross-validation |   5   | cvec__max_df                 | 0.98   |\n",
    "|    1    | |  |                            |      stratify     |   y   |  Estimator  |   'nb', MultinomialNB()   |        min_df        |                   'cvec__min_df': [1, 2]                  |                         |       | cvec__max_features           | 4000   |\n",
    "|    1    |   |  |                            |    random_state   |   42  |             |                           |        max_df        |                 'cvec__max_df': [.98, .99]                |                         |       | cvec__min_df                 | 1      |\n",
    "|    1    |                       |        |                            |                   |       |             |                           |      ngram_range     |             'cvec__ngram_range': [(1,1), (1,2)            |                         |       | cvec__ngram_range            | (1, 2) |\n",
    "|    1    |                       |        |                            |                   |       |             |                           |                      |                                                           |                         |       |                              |        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 1 Part 2\n",
    "\n",
    "**CountVectorizer (transformer)**:\n",
    "\n",
    "\n",
    "\n",
    "**Multinomial Naive Bayes (estimator)**:\n",
    "\n",
    "* Fast modeling algorithm\n",
    "* Variables are positive integers\n",
    "* Assumes all variables are independent (which is not realistic in NLP, but this assumption is accepted anyway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. CountVectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "\n",
    "pipe_cvec_nb = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())    # Naive Bayes\n",
    "])\n",
    "\n",
    "\n",
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit\n",
    "# Minimum number of documents needed to include token\n",
    "# Maximum number of documents needed to include token\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "pipe_cvec_nb_params = {\n",
    "    'cvec__max_features': [2_000, 4_000, 6_000],\n",
    "    'cvec__min_df': [1, 2],\n",
    "    'cvec__max_df': [.05, .1, .25, .4],\n",
    "    'cvec__ngram_range': [(1, 1), (1, 2)]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "\n",
    "gs_cvec_nb = GridSearchCV(pipe_cvec_nb, # what object are we optimizing?\n",
    "                  param_grid = pipe_cvec_nb_params, # what parameters values are we searching?\n",
    "                  cv=5)      # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             param_grid={'cvec__max_df': [0.05, 0.1, 0.25, 0.4],\n",
       "                         'cvec__max_features': [2000, 4000, 6000],\n",
       "                         'cvec__min_df': [1, 2],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to data.\n",
    "gs_cvec_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Grid searched:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_features': [2000, 4000, 6000],\n",
       " 'cvec__min_df': [1, 2],\n",
       " 'cvec__max_df': [0.05, 0.1, 0.25, 0.4],\n",
       " 'cvec__ngram_range': [(1, 1), (1, 2)]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Best score**: 0.8458"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Best estimator / parameters:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.4, max_features=6000,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('nb', MultinomialNB())])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.4,\n",
       " 'cvec__max_features': 6000,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_cvec__max_df</th>\n",
       "      <th>param_cvec__max_features</th>\n",
       "      <th>param_cvec__min_df</th>\n",
       "      <th>param_cvec__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.133253</td>\n",
       "      <td>0.018857</td>\n",
       "      <td>0.021290</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>0.4</td>\n",
       "      <td>6000</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'cvec__max_df': 0.4, 'cvec__max_features': 6000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 2)}</td>\n",
       "      <td>0.867769</td>\n",
       "      <td>0.834025</td>\n",
       "      <td>0.863071</td>\n",
       "      <td>0.809129</td>\n",
       "      <td>0.854772</td>\n",
       "      <td>0.845753</td>\n",
       "      <td>0.021656</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.057068</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.007757</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4000</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'cvec__max_df': 0.4, 'cvec__max_features': 4000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 2)}</td>\n",
       "      <td>0.867769</td>\n",
       "      <td>0.834025</td>\n",
       "      <td>0.871369</td>\n",
       "      <td>0.796680</td>\n",
       "      <td>0.846473</td>\n",
       "      <td>0.843263</td>\n",
       "      <td>0.027041</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.083011</td>\n",
       "      <td>0.034189</td>\n",
       "      <td>0.012905</td>\n",
       "      <td>0.004512</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6000</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'cvec__max_df': 0.25, 'cvec__max_features': 6000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 2)}</td>\n",
       "      <td>0.867769</td>\n",
       "      <td>0.825726</td>\n",
       "      <td>0.863071</td>\n",
       "      <td>0.804979</td>\n",
       "      <td>0.850622</td>\n",
       "      <td>0.842433</td>\n",
       "      <td>0.023735</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.023425</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4000</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'cvec__max_df': 0.4, 'cvec__max_features': 4000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 1)}</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>0.846473</td>\n",
       "      <td>0.863071</td>\n",
       "      <td>0.800830</td>\n",
       "      <td>0.846473</td>\n",
       "      <td>0.839964</td>\n",
       "      <td>0.020781</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.023059</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4000</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'cvec__max_df': 0.25, 'cvec__max_features': 4000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 1)}</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>0.842324</td>\n",
       "      <td>0.871369</td>\n",
       "      <td>0.800830</td>\n",
       "      <td>0.842324</td>\n",
       "      <td>0.839964</td>\n",
       "      <td>0.022530</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "45       0.133253      0.018857         0.021290        0.007331   \n",
       "41       0.057068      0.003061         0.007757        0.002968   \n",
       "33       0.083011      0.034189         0.012905        0.004512   \n",
       "40       0.023425      0.003357         0.004573        0.003183   \n",
       "28       0.023059      0.003006         0.004931        0.003755   \n",
       "\n",
       "   param_cvec__max_df param_cvec__max_features param_cvec__min_df  \\\n",
       "45                0.4                     6000                  1   \n",
       "41                0.4                     4000                  1   \n",
       "33               0.25                     6000                  1   \n",
       "40                0.4                     4000                  1   \n",
       "28               0.25                     4000                  1   \n",
       "\n",
       "   param_cvec__ngram_range  \\\n",
       "45                  (1, 2)   \n",
       "41                  (1, 2)   \n",
       "33                  (1, 2)   \n",
       "40                  (1, 1)   \n",
       "28                  (1, 1)   \n",
       "\n",
       "                                                                                                params  \\\n",
       "45   {'cvec__max_df': 0.4, 'cvec__max_features': 6000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 2)}   \n",
       "41   {'cvec__max_df': 0.4, 'cvec__max_features': 4000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 2)}   \n",
       "33  {'cvec__max_df': 0.25, 'cvec__max_features': 6000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 2)}   \n",
       "40   {'cvec__max_df': 0.4, 'cvec__max_features': 4000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 1)}   \n",
       "28  {'cvec__max_df': 0.25, 'cvec__max_features': 4000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 1)}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "45           0.867769           0.834025           0.863071   \n",
       "41           0.867769           0.834025           0.871369   \n",
       "33           0.867769           0.825726           0.863071   \n",
       "40           0.842975           0.846473           0.863071   \n",
       "28           0.842975           0.842324           0.871369   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "45           0.809129           0.854772         0.845753        0.021656   \n",
       "41           0.796680           0.846473         0.843263        0.027041   \n",
       "33           0.804979           0.850622         0.842433        0.023735   \n",
       "40           0.800830           0.846473         0.839964        0.020781   \n",
       "28           0.800830           0.842324         0.839964        0.022530   \n",
       "\n",
       "    rank_test_score  \n",
       "45                1  \n",
       "41                2  \n",
       "33                3  \n",
       "40                4  \n",
       "28                4  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printmd('**Grid searched:**')\n",
    "display(gs_cvec_nb.param_grid)\n",
    "printmd(f'**Best score**: {round(gs_cvec_nb.best_score_, 4)}')\n",
    "printmd('**Best estimator / parameters:**')\n",
    "display(gs_cvec_nb.best_estimator_)\n",
    "display(gs_cvec_nb.best_params_)\n",
    "\n",
    "pd.DataFrame(gs_cvec_nb.cv_results_).sort_values('rank_test_score').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 2\n",
    "\n",
    "**CountVectorizer (transformer)**:\n",
    "\n",
    "**TF-IDF (transformer)**:\n",
    "\n",
    "\n",
    "\n",
    "**Logistic Regression (estimator)**:\n",
    "\n",
    "* Relatively interpretable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-5c938e2b4c5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m pd.DataFrame(cvec.fit_transform(X_train).to_dense(),\n\u001b[0m\u001b[0;32m      3\u001b[0m             columns = cvec.get_feature_names())\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mof\u001b[0m \u001b[0municode\u001b[0m \u001b[0msymbols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \"\"\"\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'filename'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1326\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1327\u001b[0m             \u001b[1;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "cvec = CountVectorizer(X_train, stop_words = 'english')\n",
    "pd.DataFrame(cvec.fit_transform(X_train).to_dense(),\n",
    "            columns = cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up another pipeline\n",
    "# 1. CountVectorizer (transformer)\n",
    "# 2. TfidfVectorizer (transformer)\n",
    "# 3. LogisticRegression (estimator)\n",
    "\n",
    "pipe_log_reg_with_cvec_tfidf = Pipeline([\n",
    "    ('cvec', CountVectorizer(lowercase = False)),\n",
    "#     ('tvec', TfidfVectorizer(lowercase = False)),\n",
    "    ('log_reg', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit\n",
    "# Minimum number of documents needed to include token\n",
    "# Maximum number of documents needed to include token\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "# *****************************************************************************************************************\n",
    "# solver_list = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "# params = dict(solver=solver_list)\n",
    "# log_reg = LogisticRegression(C=1, n_jobs=-1, random_state=34)\n",
    "# clf = GridSearchCV(log_reg, params, cv=5)\n",
    "# clf.fit(X_train, y_train)\n",
    "# scores = clf.cv_results_['mean_test_score']\n",
    "\n",
    "# for score, solver in zip(scores, solver_list):\n",
    "#     print(f\"  {solver} {score:.3f}\" )\n",
    "\n",
    "# ref: https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451\n",
    "# *****************************************************************************************************************\n",
    "\n",
    "pipe_log_reg_with_cvec_tfidf_params = {\n",
    "    'cvec__max_features': [1_000, 3_000, 5_000],\n",
    "    'cvec__min_df': [1, 2],\n",
    "    'cvec__max_df': [.98, .99],\n",
    "    'cvec__ngram_range': [(1,1), (2,1)],\n",
    "    'cvec__stop_words': [None, 'english'],\n",
    "#     'tvec__max_features': [3_000, 5_000],\n",
    "#     'tvec__stop_words': [None, 'english'],\n",
    "#     'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'log_reg__C': [.001, .01, .1, 1],\n",
    "    'log_reg__penalty': ['l1', 'l2'],\n",
    "    'log_reg__max_iter': [50, 75, 100],\n",
    "    'log_reg__random_state': [42], \n",
    "    'log_reg__solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "\n",
    "gs_log_reg_with_cvec_tfidf = GridSearchCV(pipe_log_reg_with_cvec_tfidf, # what object are we optimizing?\n",
    "                  param_grid = pipe_log_reg_with_cvec_tfidf_params, # what parameters values are we searching?\n",
    "                  cv = 5)                                           # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec', CountVectorizer(lowercase=False)),\n",
       "                ('log_reg', LogisticRegression())])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to data.\n",
    "pipe_log_reg_with_cvec_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Grid searched:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_features': [1000, 3000, 5000],\n",
       " 'cvec__min_df': [1, 2],\n",
       " 'cvec__max_df': [0.98, 0.99],\n",
       " 'cvec__ngram_range': [(1, 1), (2, 1)],\n",
       " 'cvec__stop_words': [None, 'english'],\n",
       " 'log_reg__C': [0.001, 0.01, 0.1, 1],\n",
       " 'log_reg__penalty': ['l1', 'l2'],\n",
       " 'log_reg__max_iter': [50, 75, 100],\n",
       " 'log_reg__random_state': [42],\n",
       " 'log_reg__solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_score_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-9a789d8292e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprintmd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'**Grid searched:**'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgs_log_reg_with_cvec_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprintmd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'**Best score**: {round(gs_log_reg_with_cvec_tfidf.best_score_, 4)}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprintmd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'**Best estimator / parameters:**'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgs_log_reg_with_cvec_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_score_'"
     ]
    }
   ],
   "source": [
    "printmd('**Grid searched:**')\n",
    "display(gs_log_reg_with_cvec_tfidf.param_grid)\n",
    "printmd(f'**Best score**: {round(gs_log_reg_with_cvec_tfidf.best_score_, 4)}')\n",
    "printmd('**Best estimator / parameters:**')\n",
    "display(gs_log_reg_with_cvec_tfidf.best_estimator_)\n",
    "display(gs_log_reg_with_cvec_tfidf.best_params_)\n",
    "\n",
    "pd.DataFrame(gs_log_reg_with_cvec_tfidf.cv_results_).sort_values('rank_test_score').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 3\n",
    "\n",
    "Tout un ensemble\n",
    "\n",
    "https://scikit-learn.org/stable/modules/ensemble.html\n",
    "\n",
    "**CountVectorizer (transformer)**:\n",
    "\n",
    "\n",
    "\n",
    "**Random Forest * LogReg * MNBayes (estimator) **:\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 4\n",
    "\n",
    "\n",
    "**TfidfVectorizer (transformer)**\n",
    "\n",
    "\n",
    "\n",
    "**SVG (estimator)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break the pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
